indexing:
- only check key of entry if there are hash collissions
- count num entries to find perfect amount of hashmap buckets
- port flat hash map to C

compression:
- compress keys
- (maybe already fixed) fix LZ4 (de-)compression for big data (maybe the problem is about storing big data in general?)

storage:
- partially defragment database when entries deleted
- after defrag, all locations where more than ... bytes are dead get deleted from the db (entries after that moved into them)
- cache entries that are accessed a lot in memory
- endianess!

concurrency:
- use rwlock everywhere
- duplicate fp for each concurrent read  (`fdopen(dup(fd), "rb+")`)

security:
- more error checking
- improve header offset auto fix
